{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ivancanepa/sentiment-analysis-using-lstm-pytorchv3?scriptVersionId=131722969\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# **ðŸ’¡ðŸ§  Overview**\n**This Jupyter notebook presents a simple implementation of a Sentiment Analysis model. The model is trained on the IMDB dataset, which contains 50,000 movie reviews labeled as either positive or negative. We use PyTorch to build and train a recurrent neural network (RNN) with long short-term memory (LSTM) cells as our machine learning model.**","metadata":{}},{"cell_type":"markdown","source":"# **ðŸ“š Step 1: Importing Libraries and Modules**\n","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom nltk.corpus import stopwords \nfrom collections import Counter\nimport string\nimport re\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.model_selection import train_test_split","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-31T00:25:13.989259Z","iopub.execute_input":"2023-05-31T00:25:13.989537Z","iopub.status.idle":"2023-05-31T00:25:13.999988Z","shell.execute_reply.started":"2023-05-31T00:25:13.989511Z","shell.execute_reply":"2023-05-31T00:25:13.999026Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **ðŸ”Ž Step 2: Data Loading and Exploration**\n","metadata":{}},{"cell_type":"markdown","source":"Next, we load the data and take a look at its structure","metadata":{}},{"cell_type":"code","source":"\ndf = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-mov/IMDB Dataset.csv')\ndf.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2023-05-31T00:25:14.012425Z","iopub.execute_input":"2023-05-31T00:25:14.013144Z","iopub.status.idle":"2023-05-31T00:25:14.5652Z","shell.execute_reply.started":"2023-05-31T00:25:14.013093Z","shell.execute_reply":"2023-05-31T00:25:14.564085Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We first check if a GPU is available for training our model. If it's not, we will use the CPU\nis_cuda = torch.cuda.is_available()\n\n# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\nif is_cuda:\n    device = torch.device(\"cuda\")\n    print(\"GPU is available\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU not available, CPU used\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **ðŸ“‹ Step 3: Data Preprocessing**","metadata":{}},{"cell_type":"markdown","source":"Data preprocessing is crucial to convert the raw text into a format that can be consumed by our machine learning model.\n\nFirst, we split the data into training and testing sets to avoid data leakage. This is done using the train_test_split function from sklearn.","metadata":{}},{"cell_type":"code","source":"X,y = df['review'].values,df['sentiment'].values\nx_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y)\nprint(f'shape of train data is {x_train.shape}')\nprint(f'shape of test data is {x_test.shape}')","metadata":{"execution":{"iopub.status.busy":"2023-05-31T00:25:14.566785Z","iopub.execute_input":"2023-05-31T00:25:14.567245Z","iopub.status.idle":"2023-05-31T00:25:14.664688Z","shell.execute_reply.started":"2023-05-31T00:25:14.567211Z","shell.execute_reply":"2023-05-31T00:25:14.663623Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We then analyze the sentiment distribution in the training set to get an idea of the balance between positive and negative reviews.","metadata":{}},{"cell_type":"code","source":"dd = pd.Series(y_train).value_counts()\nsns.barplot(x=np.array(['negative','positive']),y=dd.values)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-31T00:40:01.063474Z","iopub.execute_input":"2023-05-31T00:40:01.064359Z","iopub.status.idle":"2023-05-31T00:40:01.266955Z","shell.execute_reply.started":"2023-05-31T00:40:01.06432Z","shell.execute_reply":"2023-05-31T00:40:01.266014Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ðŸ”¬ 3.a Tockenization**\n","metadata":{}},{"cell_type":"code","source":"def preprocess_string(s):\n    # Remove all non-word characters (everything except numbers and letters)\n    s = re.sub(r\"[^\\w\\s]\", '', s)\n    # Replace all runs of whitespaces with no space\n    s = re.sub(r\"\\s+\", '', s)\n    # replace digits with no space\n    s = re.sub(r\"\\d\", '', s)\n\n    return s\n\ndef tockenize(x_train,y_train,x_val,y_val):\n    word_list = []\n\n    stop_words = set(stopwords.words('english')) \n    for sent in x_train:\n        for word in sent.lower().split():\n            word = preprocess_string(word)\n            if word not in stop_words and word != '':\n                word_list.append(word)\n  \n    corpus = Counter(word_list)\n    # sorting on the basis of most common words\n    corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:1000]\n    # creating a dict\n    onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}\n    \n    # tockenize\n    final_list_train,final_list_test = [],[]\n    for sent in x_train:\n            final_list_train.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n                                     if preprocess_string(word) in onehot_dict.keys()])\n    for sent in x_val:\n            final_list_test.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n                                    if preprocess_string(word) in onehot_dict.keys()])\n            \n    encoded_train = [1 if label =='positive' else 0 for label in y_train]  \n    encoded_test = [1 if label =='positive' else 0 for label in y_val] \n    return np.array(final_list_train), np.array(encoded_train),np.array(final_list_test), np.array(encoded_test),onehot_dict\n","metadata":{"execution":{"iopub.status.busy":"2023-05-31T00:25:14.882717Z","iopub.execute_input":"2023-05-31T00:25:14.883084Z","iopub.status.idle":"2023-05-31T00:25:14.89518Z","shell.execute_reply.started":"2023-05-31T00:25:14.883048Z","shell.execute_reply":"2023-05-31T00:25:14.894161Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tokenization is another critical step where we convert the text into tokens (words, in this case). We also encode our labels, converting 'positive' and 'negative' into binary numerical values","metadata":{}},{"cell_type":"code","source":"x_train,y_train,x_test,y_test,vocab = tockenize(x_train,y_train,x_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T00:25:14.896471Z","iopub.execute_input":"2023-05-31T00:25:14.896872Z","iopub.status.idle":"2023-05-31T00:27:09.132627Z","shell.execute_reply.started":"2023-05-31T00:25:14.896839Z","shell.execute_reply":"2023-05-31T00:27:09.131628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Length of vocabulary is {len(vocab)}')","metadata":{"execution":{"iopub.status.busy":"2023-05-31T00:27:09.137513Z","iopub.execute_input":"2023-05-31T00:27:09.139931Z","iopub.status.idle":"2023-05-31T00:27:09.147089Z","shell.execute_reply.started":"2023-05-31T00:27:09.139872Z","shell.execute_reply":"2023-05-31T00:27:09.146178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ðŸ“‰ 3.b Analysing review length**","metadata":{}},{"cell_type":"code","source":"rev_len = [len(i) for i in x_train]\npd.Series(rev_len).hist()\nplt.show()\npd.Series(rev_len).describe()","metadata":{"execution":{"iopub.status.busy":"2023-05-31T00:27:09.148863Z","iopub.execute_input":"2023-05-31T00:27:09.15025Z","iopub.status.idle":"2023-05-31T00:27:09.884194Z","shell.execute_reply.started":"2023-05-31T00:27:09.150208Z","shell.execute_reply":"2023-05-31T00:27:09.883185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations : <br>a) Mean review length = around 69.<br> b) minimum length of reviews is 2.<br>c)There are quite a few reviews that are extremely long, we can manually investigate them to check whether we need to include or exclude them from our analysis.","metadata":{}},{"cell_type":"markdown","source":"**ðŸ’» 3.c Padding**\n\nNow we will pad each of the sequence to max length ","metadata":{}},{"cell_type":"code","source":"def padding_(sentences, seq_len):\n    features = np.zeros((len(sentences), seq_len),dtype=int)\n    for ii, review in enumerate(sentences):\n        if len(review) != 0:\n            features[ii, -len(review):] = np.array(review)[:seq_len]\n    return features","metadata":{"execution":{"iopub.status.busy":"2023-05-31T00:27:09.894Z","iopub.execute_input":"2023-05-31T00:27:09.896645Z","iopub.status.idle":"2023-05-31T00:27:09.906806Z","shell.execute_reply.started":"2023-05-31T00:27:09.896591Z","shell.execute_reply":"2023-05-31T00:27:09.9057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we have very less number of reviews with length > 500.\n#So we will consideronly those below it.\nx_train_pad = padding_(x_train,500)\nx_test_pad = padding_(x_test,500)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-31T00:27:09.908626Z","iopub.execute_input":"2023-05-31T00:27:09.909424Z","iopub.status.idle":"2023-05-31T00:27:10.822723Z","shell.execute_reply.started":"2023-05-31T00:27:09.909388Z","shell.execute_reply":"2023-05-31T00:27:10.821767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ðŸ“š 3.d Batching and loading as tensor**","metadata":{}},{"cell_type":"code","source":"# create Tensor datasets\ntrain_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\nvalid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n\n# dataloaders\nbatch_size = 50\n\n# make sure to SHUFFLE your data\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\nvalid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-05-31T00:27:10.826968Z","iopub.execute_input":"2023-05-31T00:27:10.827273Z","iopub.status.idle":"2023-05-31T00:27:10.835884Z","shell.execute_reply.started":"2023-05-31T00:27:10.827246Z","shell.execute_reply":"2023-05-31T00:27:10.834956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# obtain one batch of training data\ndataiter = iter(train_loader)\nsample_x, sample_y = next(dataiter)\n\nprint('Sample input size: ', sample_x.size()) # batch_size, seq_length\nprint('Sample input: \\n', sample_x)\nprint('Sample input: \\n', sample_y)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-31T00:27:10.837063Z","iopub.execute_input":"2023-05-31T00:27:10.837727Z","iopub.status.idle":"2023-05-31T00:27:10.853618Z","shell.execute_reply.started":"2023-05-31T00:27:10.837691Z","shell.execute_reply":"2023-05-31T00:27:10.852607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to add an embedding layer because there are less words in our vocabulary. It is massively inefficient to one-hot encode that many classes. So, instead of one-hot encoding, we can have an embedding layer and use that layer as a lookup table. You could train an embedding layer using Word2Vec, then load it here. But, it's fine to just make a new layer, using it for only dimensionality reduction, and let the network learn the weights.","metadata":{}},{"cell_type":"markdown","source":"# **ðŸ§  Step 4 Model Definition**","metadata":{}},{"cell_type":"markdown","source":"**We then move on to defining our machine learning model. We are using an RNN with LSTM cells.**","metadata":{}},{"cell_type":"code","source":"class SentimentRNN(nn.Module):\n    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n        super(SentimentRNN,self).__init__()\n \n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n \n        self.no_layers = no_layers\n        self.vocab_size = vocab_size\n    \n        # embedding and LSTM layers\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        \n        #lstm\n        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n                           num_layers=no_layers, batch_first=True)\n        \n        \n        # dropout layer\n        self.dropout = nn.Dropout(0.3)\n    \n        # linear and sigmoid layer\n        self.fc = nn.Linear(self.hidden_dim, output_dim)\n        self.sig = nn.Sigmoid()\n        \n    def forward(self,x,hidden):\n        batch_size = x.size(0)\n        # embeddings and lstm_out\n        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n        #print(embeds.shape)  #[50, 500, 1000]\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        \n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n        \n        # dropout and fully connected layer\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        \n        # sigmoid function\n        sig_out = self.sig(out)\n        \n        # reshape to be batch_size first\n        sig_out = sig_out.view(batch_size, -1)\n\n        sig_out = sig_out[:, -1] # get last batch of labels\n        \n        # return last sigmoid output and hidden state\n        return sig_out, hidden\n        \n        \n        \n    def init_hidden(self, batch_size):\n        ''' Initializes hidden state '''\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n        hidden = (h0,c0)\n        return hidden\n\n              ","metadata":{"execution":{"iopub.status.busy":"2023-05-31T00:27:10.856438Z","iopub.execute_input":"2023-05-31T00:27:10.857095Z","iopub.status.idle":"2023-05-31T00:27:10.869236Z","shell.execute_reply.started":"2023-05-31T00:27:10.85706Z","shell.execute_reply":"2023-05-31T00:27:10.868363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_layers = 2\nvocab_size = len(vocab) + 1 #extra 1 for padding\nembedding_dim = 64\noutput_dim = 1\nhidden_dim = 256\n\n\nmodel = SentimentRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n\n#moving to gpu\nmodel.to(device)\n\nprint(model)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-31T00:27:10.872191Z","iopub.execute_input":"2023-05-31T00:27:10.873099Z","iopub.status.idle":"2023-05-31T00:27:17.194535Z","shell.execute_reply.started":"2023-05-31T00:27:10.873061Z","shell.execute_reply":"2023-05-31T00:27:17.193366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **ðŸ“ˆ Step 5: Training and Validation**","metadata":{}},{"cell_type":"markdown","source":"**Now, we are ready to train our model. We will define a loss function and an optimizer for this purpose. We're using the Binary Cross-Entropy Loss (also known as Log Loss) as the loss function and Adam as our optimizer.**","metadata":{}},{"cell_type":"code","source":"# loss and optimization functions\nlr=0.001\n\ncriterion = nn.BCELoss()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n# function to predict accuracy\ndef acc(pred,label):\n    pred = torch.round(pred.squeeze())\n    return torch.sum(pred == label.squeeze()).item()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-31T00:27:17.196136Z","iopub.execute_input":"2023-05-31T00:27:17.19652Z","iopub.status.idle":"2023-05-31T00:27:17.202893Z","shell.execute_reply.started":"2023-05-31T00:27:17.196484Z","shell.execute_reply":"2023-05-31T00:27:17.201739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Next, we start training the model for a certain number of epochs.**\n\n","metadata":{}},{"cell_type":"code","source":"clip = 5\nepochs = 5 \nvalid_loss_min = np.Inf\n# train for some number of epochs\nepoch_tr_loss,epoch_vl_loss = [],[]\nepoch_tr_acc,epoch_vl_acc = [],[]\n\nfor epoch in range(epochs):\n    train_losses = []\n    train_acc = 0.0\n    model.train()\n    # initialize hidden state \n    h = model.init_hidden(batch_size)\n    for inputs, labels in train_loader:\n        \n        inputs, labels = inputs.to(device), labels.to(device)   \n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n        \n        model.zero_grad()\n        output,h = model(inputs,h)\n        \n        # calculate the loss and perform backprop\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        train_losses.append(loss.item())\n        # calculating accuracy\n        accuracy = acc(output,labels)\n        train_acc += accuracy\n        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n \n    \n        \n    val_h = model.init_hidden(batch_size)\n    val_losses = []\n    val_acc = 0.0\n    model.eval()\n    for inputs, labels in valid_loader:\n            val_h = tuple([each.data for each in val_h])\n\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            output, val_h = model(inputs, val_h)\n            val_loss = criterion(output.squeeze(), labels.float())\n\n            val_losses.append(val_loss.item())\n            \n            accuracy = acc(output,labels)\n            val_acc += accuracy\n            \n    epoch_train_loss = np.mean(train_losses)\n    epoch_val_loss = np.mean(val_losses)\n    epoch_train_acc = train_acc/len(train_loader.dataset)\n    epoch_val_acc = val_acc/len(valid_loader.dataset)\n    epoch_tr_loss.append(epoch_train_loss)\n    epoch_vl_loss.append(epoch_val_loss)\n    epoch_tr_acc.append(epoch_train_acc)\n    epoch_vl_acc.append(epoch_val_acc)\n    print(f'Epoch {epoch+1}') \n    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n    if epoch_val_loss <= valid_loss_min:\n        torch.save(model.state_dict(), '../working/state_dict.pt')\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n        valid_loss_min = epoch_val_loss\n    print(25*'==')\n    ","metadata":{"execution":{"iopub.status.busy":"2023-05-31T00:27:17.20453Z","iopub.execute_input":"2023-05-31T00:27:17.204862Z","iopub.status.idle":"2023-05-31T00:32:22.89136Z","shell.execute_reply.started":"2023-05-31T00:27:17.204825Z","shell.execute_reply":"2023-05-31T00:32:22.890156Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (20, 6))\nplt.subplot(1, 2, 1)\nplt.plot(epoch_tr_acc, label='Train Acc')\nplt.plot(epoch_vl_acc, label='Validation Acc')\nplt.title(\"Accuracy\")\nplt.legend()\nplt.grid()\n    \nplt.subplot(1, 2, 2)\nplt.plot(epoch_tr_loss, label='Train loss')\nplt.plot(epoch_vl_loss, label='Validation loss')\nplt.title(\"Loss\")\nplt.legend()\nplt.grid()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-31T00:32:22.892911Z","iopub.execute_input":"2023-05-31T00:32:22.893788Z","iopub.status.idle":"2023-05-31T00:32:23.610105Z","shell.execute_reply.started":"2023-05-31T00:32:22.893758Z","shell.execute_reply":"2023-05-31T00:32:23.609195Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **ðŸ“ Step 6: Evaluation and Inference**\n**After training, we load the best model based on validation loss and test it on our unseen test data.**","metadata":{}},{"cell_type":"code","source":"def predict_text(text):\n        word_seq = np.array([vocab[preprocess_string(word)] for word in text.split() \n                         if preprocess_string(word) in vocab.keys()])\n        word_seq = np.expand_dims(word_seq,axis=0)\n        pad =  torch.from_numpy(padding_(word_seq,500))\n        inputs = pad.to(device)\n        batch_size = 1\n        h = model.init_hidden(batch_size)\n        h = tuple([each.data for each in h])\n        output, h = model(inputs, h)\n        return(output.item())","metadata":{"execution":{"iopub.status.busy":"2023-05-31T00:32:23.615274Z","iopub.execute_input":"2023-05-31T00:32:23.618501Z","iopub.status.idle":"2023-05-31T00:32:23.628299Z","shell.execute_reply.started":"2023-05-31T00:32:23.618463Z","shell.execute_reply":"2023-05-31T00:32:23.627146Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nindex = 30\nprint(df['review'][index])\nprint('='*70)\nprint(f'Actual sentiment is  : {df[\"sentiment\"][index]}')\nprint('='*70)\npro = predict_text(df['review'][index])\nstatus = \"positive\" if pro > 0.5 else \"negative\"\npro = (1 - pro) if status == \"negative\" else pro\nprint(f'Predicted sentiment is {status} with a probability of {pro}')","metadata":{"execution":{"iopub.status.busy":"2023-05-31T00:32:23.633582Z","iopub.execute_input":"2023-05-31T00:32:23.637088Z","iopub.status.idle":"2023-05-31T00:32:23.669515Z","shell.execute_reply.started":"2023-05-31T00:32:23.63705Z","shell.execute_reply":"2023-05-31T00:32:23.668557Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nindex = 32\nprint(df['review'][index])\nprint('='*70)\nprint(f'Actual sentiment is  : {df[\"sentiment\"][index]}')\nprint('='*70)\npro = predict_text(df['review'][index])\nstatus = \"positive\" if pro > 0.5 else \"negative\"\npro = (1 - pro) if status == \"negative\" else pro\nprint(f'predicted sentiment is {status} with a probability of {pro}')","metadata":{"execution":{"iopub.status.busy":"2023-05-31T00:32:23.673597Z","iopub.execute_input":"2023-05-31T00:32:23.67594Z","iopub.status.idle":"2023-05-31T00:32:23.712536Z","shell.execute_reply.started":"2023-05-31T00:32:23.675904Z","shell.execute_reply":"2023-05-31T00:32:23.711498Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some improvement suggestions are as follow:\n\n* Running a hyperparameter search to optimize your configurations.\n* Using pretraned word embeddings like Glove word embeddings\n* Increasing the model complexity like adding more layers/ using bidirectional LSTMs\n","metadata":{}}]}